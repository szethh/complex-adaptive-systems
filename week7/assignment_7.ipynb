{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 8: Policy gradients in Jax"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment, you will implement the REINFORCE algorithm for the inverted pendulum swing-up problem. The pendulum starts in a random position and the goal is to apply torque on the free end to swing it into an upright position, with its center of gravity right above the fixed point. \n",
    "\n",
    "Like in the previous assignment, we will use the Jax framework for automatic differentiation. We (again) recommend you to look through the following articles to get started with the Jax framework:\n",
    "\n",
    "- [Jax Quickstart](https://jax.readthedocs.io/en/latest/notebooks/quickstart.html)\n",
    "- [Training a simple NN with Jax](https://colab.research.google.com/github/google/jax/blob/main/docs/notebooks/neural_network_with_tfds_data.ipynb)\n",
    "- [Jax vs. NumPy](https://jax.readthedocs.io/en/latest/notebooks/thinking_in_jax.html)\n",
    "\n",
    "In addition, we use two additional libraries, optax and gymnax, which are used for optimization and for simulating OpenAI gym models. Both packages can be installed via the pip package manager."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.1 Using gymnax (0 points)\n",
    "the following code sets up a gymnax environment for the inverted pendulum swing up, where we discretize the action space manually. The code below does not have to be changed, but it is recommend to try to understand observation and state representations of the pendulum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import jax.random as jr\n",
    "import jax.nn as jnn\n",
    "from jax.random import PRNGKey\n",
    "import optax\n",
    "import gymnax\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = PRNGKey(42)\n",
    "key, key_reset, key_policy, key_step = jr.split(key, 4)\n",
    "\n",
    "# Create the Pendulum-v1 environment\n",
    "env_name = \"Pendulum-v1\"\n",
    "env, env_params = gymnax.make(env_name)\n",
    "\n",
    "# Inspect default environment settings\n",
    "print(env_params)\n",
    "ts = jnp.arange(0, env_params.dt * env_params.max_steps_in_episode, env_params.dt)\n",
    "\n",
    "obs, state = env.reset(key_reset, env_params)\n",
    "obs, state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action = env.action_space(env_params).sample(key_policy)\n",
    "action_scalar = env.action_space() \n",
    "n_obs, n_state, reward, done, _ = env.step(key_step, state, action, env_params)\n",
    "n_obs, n_state, reward, done"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this assignment, we discretize the action space:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs, state = env.reset(key_reset)\n",
    "action = env.action_space().sample(key_policy)\n",
    "action_list = jnp.array([-1., 0., 1.]) \n",
    "num_actions = len(action_list)\n",
    "n_obs, n_state, reward, done, _ = env.step(key_step, state, action)\n",
    "\n",
    "env_params"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.2 Setting up a basic neural network (3 points)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a basic neural network in Jax using jax.nn and jax.random for initialising the weights and biases randomly with a mean of zero and a small standard deviation. \n",
    "\n",
    "Since the network represents the policy, it should take a state as input and output a predicted probability distribution over all discrete actions. Below is a suggested setup for the functions via which this can be done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_mlp(layer_sizes, key:PRNGKey, scale:float=1e-2):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "        layer_sizes (tuple) Tuple of shapes of the neural network layers. Includes the input shape, hidden layer shape, and output layer shape.\n",
    "        key (PRNGKey) \n",
    "        scale (float) standard deviation of initial weights and biases\n",
    "\n",
    "    Return: \n",
    "        params (List) Tuple of weights and biases - [ (weights_1, biases_1), ..., (weights_n, biases_n) ]\n",
    "    \"\"\"\n",
    "    raise NotImplementedError\n",
    "\n",
    "def policy(params, x):\n",
    "    \"\"\" Standard MLP that predicts either -1, 0, 1.\n",
    "    \n",
    "    Inputs:\n",
    "        params (PyTree) Parameters of the policy network, represented as PyTree. \n",
    "        x (D,) input state, where D is the dimensionality of the state observation.\n",
    "        \"\"\"\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to the policy network, we will need some helper functions to do the following: \n",
    "\n",
    "- Sample an action from the policy distribution.\n",
    "- Calculate the log probability of an action given the policy distribution.\n",
    "- Update the error terms $\\delta$ during training (hint: the tree.map() function from question 1 is useful here)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_action(params, x, key:PRNGKey):\n",
    "    \"\"\"  \n",
    "    Sample an action using the action probabilities predicted by the MLP\n",
    "\n",
    "    Input:\n",
    "        params (PyTree) Parameters of the policy network, represented as PyTree. \n",
    "        x (D,) input state, where D is the dimensionality of the state observation.\n",
    "        key (PRNGKey)\n",
    "     \n",
    "    Return:\n",
    "        action (M,) of floats: actions generated according to params, where M is the dimensionality of actions we carry out. \n",
    "        action_idx (M,) of int: indices of actions generated according to params.\n",
    "    \"\"\"\n",
    "    return 0., 0\n",
    " \n",
    "def get_log_prob(params, x, action_idx):\n",
    "    \"\"\"\n",
    "    Return the log probability of the action executed by the MLP.\n",
    "\n",
    "    Input:\n",
    "        params (PyTree) Parameters of the policy network, represented as PyTree. \n",
    "        x (D,) input state, where D is the dimensionality of the state observation.\n",
    "        action_idx (M,) of int: indices of actions generated according to params.\n",
    "     \n",
    "    Return:\n",
    "        log probability\n",
    "    \"\"\"\n",
    "    raise NotImplementedError\n",
    "\n",
    "@jax.jit\n",
    "def update_delta(delta, grad_theta):\n",
    "    \"\"\" \n",
    "    Update the parameter update delta with the gradient of the policy.\n",
    "\n",
    "    Input:\n",
    "        delta (PyTree) current loss term\n",
    "        grad_theta (PyTree) gradient update of the network parameters.\n",
    "    \n",
    "    Return:\n",
    "        updated_delta (PyTree) \n",
    "    \"\"\"\n",
    "    updated_delta = None\n",
    "    raise NotImplementedError\n",
    "    return updated_delta, None # you can leave the second return (None) for functionality of training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the policy \n",
    "params = None"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intermezzo: generating rollouts of the pendulum in parallel with Gymnax.\n",
    "The following code generates batched rollouts of the environment in parallel. Note that it requires the 'get_action' function that should have been implemented above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rollout(params, env_params, rng_input:PRNGKey, steps_in_episode:int):\n",
    "    \"\"\"Rollout a jitted gymnax episode with lax.scan.\"\"\"\n",
    "    # Reset the environment\n",
    "    rng_reset, rng_episode = jr.split(rng_input)\n",
    "    obs, state = env.reset(rng_reset, env_params)\n",
    "\n",
    "\n",
    "    def policy_step(state_input, tmp):\n",
    "        \"\"\"lax.scan compatible step transition in jax env.\"\"\"\n",
    "        obs, state, rng = state_input\n",
    "        rng, rng_action, rng_step = jr.split(rng, 3)\n",
    "        action, action_idx = get_action(params, obs, rng_action)\n",
    "        next_obs, next_state, reward, done, _ = env.step(\n",
    "          rng_step, state, action, env_params\n",
    "        )\n",
    "        carry = [next_obs, next_state, rng]\n",
    "        return carry, [obs, state, action, action_idx, reward, next_obs, done]\n",
    "\n",
    "    # Scan over episode step loop\n",
    "    _, scan_out = jax.lax.scan(\n",
    "      policy_step,\n",
    "      [obs, state, rng_episode],\n",
    "      (),\n",
    "      length=steps_in_episode, \n",
    "    )\n",
    "    return scan_out\n",
    "\n",
    "# Jit-Compiled Episode Rollout\n",
    "jit_rollout = jax.jit(rollout, static_argnums=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_trajectory(params, key):\n",
    "    obs, state, action, action_idx, reward, next_obs, done = rollout(params, env_params, rng_input=key, steps_in_episode=env_params.max_steps_in_episode)\n",
    "\n",
    "\n",
    "    fig, ax = plt.subplots(5,1,figsize=(8,8))\n",
    "    # first three plots for the system states\n",
    "    ax[0].set_title('System states over time')\n",
    "\n",
    "    for d in range(env.obs_shape[0]):\n",
    "        ax[d].plot(ts, obs[:,d], color='C0', label=f'State {d}')\n",
    "    ax[0].set_title(r'$\\cos(\\theta)$')\n",
    "    ax[1].set_title(r'$\\sin(\\theta)$')\n",
    "    ax[2].set_title(r'$\\dot{\\theta}$')\n",
    "    \n",
    "    ax[3].plot(ts, action, color='C1', label=f'Actions')\n",
    "    # ax[3].set_ylim((env.action_space().low, env.action_space().high))\n",
    "    ax[3].set_title('u(t)')\n",
    "    ax[4].plot(ts, reward, color='C2', label='Rewards')\n",
    "    ax[4].set_title('r(t)')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key, subkey = jr.split(key)\n",
    "print('Caption 1: Single rollout using the policy network without training')\n",
    "visualize_trajectory(params,key = subkey)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.3 REINFORCE without baseline (4 points)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement the REINFORCE algorithm from the lecture notes (algorithm 1) to compute the error terms to update the parameters. Make use of get_log_prob and update_delta functions implemented earlier. \n",
    "\n",
    "For iterating over the time steps, we recommend using the jax.lax.scan function. To parallelize the gradients across batches, it is recommended to use jax.vmap function. Note that either of them are possible using for loops too, however using these methods will drastically speed up the code.\n",
    "\n",
    "To get you started, the training loop is already defined below. As such, you will have to implemenet the REINFORCE loss and choose the learning parameters. Keep in mind that optax.optimizer assumes a minimization problem when computing the gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_REINFORCE(params, obs, action_idx, reward, baseline, gamma:float=0.99):   \n",
    "    \"\"\"\n",
    "    Compute the error term delta using the REINFORCE algorithm\n",
    "\n",
    "    Inputs:\n",
    "        params (PyTree) Current parameters of the network\n",
    "        obs (Array) Batch of observations\n",
    "        action_idx (Array) Batch of action indices\n",
    "        reward (Array) Batch of rewards\n",
    "        baseline (Array) Baseline over time points - not required for current question 8.3.\n",
    "\n",
    "    Return:\n",
    "        delta (PyTree) Error terms of the parameters\n",
    "        Gt (Array) Batched discounted rewards over time\n",
    "    \"\"\"\n",
    "\n",
    "    def trajectory_gradients(reward, obs, action_idx, baseline, delta): \n",
    "        G_init = 0\n",
    "\n",
    "        def step(carry, variables):\n",
    "            G, delta = carry\n",
    "            r, obs, action_idx, baseline = variables\n",
    "            \n",
    "            \"\"\"\n",
    "            YOUR CODE HERE\n",
    "            \"\"\"\n",
    "\n",
    "            carry = G, delta\n",
    "            return carry, G\n",
    "\n",
    "        #Iterate backwards in time\n",
    "        variables = (reward[::-1], \n",
    "                     obs[::-1], \n",
    "                     action_idx[::-1], \n",
    "                     baseline[::-1])\n",
    "        \n",
    "        \"\"\" WRITE YOUR SCAN FUNCTION HERE THAT CALLS step()\"\"\"\n",
    "        (_, delta), Gt = jax.lax.scan(None)\n",
    "        return delta, Gt\n",
    "\n",
    "    # create a parallizable function and initialize the error terms delta.\n",
    "    \"\"\" VMAP THE trajectory_gradients() OVER THE BATCH SIZE\"\"\"\n",
    "    parallel_trajectory_gradients = jax.vmap(None)\n",
    "\n",
    "    # compute the delta gradients in parallel and sum them up.\n",
    "    delta = jax.tree.map(lambda t: jnp.zeros(t.shape), params)\n",
    "    deltas, Gs = parallel_trajectory_gradients(reward, obs, action_idx, baseline, delta)    \n",
    "    delta, _ = jax.lax.scan(update_delta, delta, deltas)\n",
    "\n",
    "    return delta, jnp.array(Gs)\n",
    "\n",
    "# Jit the function for computational efficiency. Note: for printing inside the function, do not jit this by commenting the below line.\n",
    "loss_REINFORCE = jax.jit(loss_REINFORCE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set training parameters\n",
    "num_iters = None\n",
    "steps_in_episode = env_params.max_steps_in_episode\n",
    "lr = None\n",
    "gamma = None\n",
    "n_batches = None\n",
    "\n",
    "optim = optax.adam(learning_rate=lr)\n",
    "state = optim.init(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mini-batch random keys to scan over.\n",
    "key, subkey = jr.split(key) \n",
    "iter_keys = jr.split(subkey, num_iters)\n",
    "\n",
    "\n",
    "# Optimisation step.\n",
    "def step(carry, key):\n",
    "    params, opt_state, env_params = carry\n",
    "    \n",
    "    # forward pass\n",
    "    keys = jr.split(key, n_batches)\n",
    "    parallel_rollout = jax.vmap(rollout, in_axes=(None,None,0,None))\n",
    "    obs, _, action, action_idx, reward, next_obs, done = parallel_rollout(params, \n",
    "                                                         env_params, \n",
    "                                                         keys,\n",
    "                                                         steps_in_episode)\n",
    "    empty_baseline = jnp.zeros((reward.shape[-1])) \n",
    "\n",
    "    # compute gradients and update model\n",
    "    delta, _ = loss_REINFORCE(params, obs, action_idx, reward, empty_baseline, gamma)\n",
    "    updates, opt_state = optim.update(delta, opt_state, params)\n",
    "    new_params = optax.apply_updates(params, updates)\n",
    "\n",
    "    carry = new_params, opt_state, env_params\n",
    "    return carry, jnp.mean(jnp.sum(reward,axis=-1))\n",
    "\n",
    "# Optimisation loop.\n",
    "(params, _, _), history = jax.lax.scan(step, (params, state, env_params), (iter_keys))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history, label='loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_trajectory(params, subkey)\n",
    "key, subkey = jr.split(key)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.4 Create (time-dependent) baseline (3 points)\n",
    "\n",
    "Implement a baseline $b(t) = E[ G_t] $ following Algorithm 2 in the lecture notes. Rather than minimizing $| G_n - b(x_n)|^2$, you can instead take the average discounted reward as a constant baseline, or compute the time-dependent baseline by averaging over the cumulative discounted rewards over time.\n",
    "\n",
    "Hint: this should be possible by only altering the training loop function, without altering the REINFORCE loss function. \n",
    "\n",
    "Comment on the effect of adding the baseline, and explain how this impacts the training of the network."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jax-cpu2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
